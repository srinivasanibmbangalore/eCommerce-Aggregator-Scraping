{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CheckForRobottxt Policy .ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP1+1i85w8ZheYYvtcJPukD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srinivasanibmbangalore/eCommerce-Aggregator-Scraping/blob/master/CheckForRobottxt_Policy_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5999u_IoXE7",
        "colab_type": "text"
      },
      "source": [
        "# Do the websiteâ€™s owners allow legal scraping? \n",
        "It is important to find out,read the Terms & Conditions and the Privacy Policy of the website before scraping them. Most websites provide a file called robots.txt, which is used to tell web\n",
        "crawlers what they can scrape and what they should not touch.\n",
        "\n",
        "Python's built-in module  \"robotparser\", \n",
        "enables us to read and understand the robots.txt file. It helps in determining if\n",
        "we are allowed to scrape a given part of the target website\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta_VGyn7tFjL",
        "colab_type": "text"
      },
      "source": [
        "***Spiders created using Scrapy 1.1+  respect robots.txt by default.*** This can be disabled by setting the variable ROBOTSTXT_OBEY = False.  In that case, scrapy does not check the robots.txt file. It will start crawling the URLs specified the start_urls list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LylSMcVt07e",
        "colab_type": "text"
      },
      "source": [
        "This is a piece of code which reads the robots.txt policy of a website and deciphers as to whether crawling/scraping is allowed. Here we are looking at alibaba.com as an example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2-kaqjTriVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib import robotparser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK7ivDRJpL1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "__author__ = \"Srinivasan Subramaniam\"\n",
        "__copyright__ = \"None\"\n",
        "__credits__ = [\"Multiple Books on Scrapy and Beautiful Soup\"]\n",
        "__license__ = \"GPL\"\n",
        "__version__ = \"1.0.0\"\n",
        "__maintainer__ = \"Srinivasan Subramaniam\"\n",
        "__email__ = \"srinivasan.ibmbangalore@gmail.com\"\n",
        "__status__ = \"Production\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8nIxvdNrsOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "robot_parser =  robotparser.RobotFileParser() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJfRS8rQvYoe",
        "colab_type": "text"
      },
      "source": [
        "Read the robot.txt parser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBc2ln84vWe7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readrbottxtfile (robottxturl):\n",
        "  robot_parser.set_url(robottxturl)\n",
        "  robot_parser.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2tafdZKwGer",
        "colab_type": "text"
      },
      "source": [
        "A function to check if the target url can be read as per Robot.txt policy of alibaba.com"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elaosRhEv5wX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def is_allowed(target_url, user_agent='*'): # This looks into the section user_agent *\n",
        " return robot_parser.can_fetch(user_agent, target_url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCHuWQjNwQDF",
        "colab_type": "code",
        "outputId": "148f614e-17d6-4592-f209-21ce3ea8d0ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "robottxturl=\"http://alibaba.com/robots.txt\"\n",
        "urltobecrawled=\"https://www.alibaba.com/trade/search?fsb=y&IndexArea=product_en&CatId=&SearchText=earphones&viewtype=G\" \n",
        "\n",
        "readrbottxtfile(robottxturl)\n",
        "isAllowed= is_allowed(urltobecrawled)\n",
        "if (isAllowed == True):\n",
        "  print(\"Crawling of Alibaba.com for product price search is allowed \")\n",
        "else:\n",
        "  print(\"Crawling of Alibaba.com for product price search is not allowed\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Crawling of Alibaba.com for product price search is not allowed\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}